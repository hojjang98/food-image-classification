{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53fd73ac",
   "metadata": {},
   "source": [
    "ðŸ”§ 1. Import & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d110e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import urllib.request\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ede13d",
   "metadata": {},
   "source": [
    "ðŸ•·ï¸ 2. Define the Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5fa7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_images(keyword, save_dir, count=200, delay=1.0):\n",
    "    \"\"\"\n",
    "    Download images from Google Image Search using Selenium.\n",
    "    \n",
    "    Parameters:\n",
    "    - keyword: Search term\n",
    "    - save_dir: Directory to save images\n",
    "    - count: Number of images to collect\n",
    "    - delay: Delay between image loads\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Headless browser setup\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    # Launch Chrome driver\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(\"https://www.google.com/imghp?hl=en\")\n",
    "\n",
    "    # Search the keyword\n",
    "    search_box = driver.find_element(By.NAME, \"q\")\n",
    "    search_box.send_keys(keyword)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Scroll to load images\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(delay)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            try:\n",
    "                driver.find_element(By.CLASS_NAME, \"mye4qd\").click()\n",
    "            except:\n",
    "                break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Find image elements\n",
    "    images = driver.find_elements(By.CSS_SELECTOR, \"img.rg_i\")\n",
    "\n",
    "    # Download images\n",
    "    downloaded = 0\n",
    "    for img in images:\n",
    "        if downloaded >= count:\n",
    "            break\n",
    "        try:\n",
    "            img.click()\n",
    "            time.sleep(delay)\n",
    "            actual_images = driver.find_elements(By.CSS_SELECTOR, \"img.n3VNCb\")\n",
    "            for actual_img in actual_images:\n",
    "                src = actual_img.get_attribute(\"src\")\n",
    "                if src and \"http\" in src:\n",
    "                    try:\n",
    "                        filename = os.path.join(save_dir, f\"{keyword}_{downloaded}.jpg\")\n",
    "                        urllib.request.urlretrieve(src, filename)\n",
    "                        downloaded += 1\n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45bcb3c",
   "metadata": {},
   "source": [
    "ðŸš€ 3. Start Crawling by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf2e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categories (modify as needed)\n",
    "categories = [\n",
    "    \"fruit apple\", \"fruit banana\", \"fruit kiwi\", \"fruit pineapple\",\n",
    "    \"vegetable carrot\", \"vegetable cabbage\", \"vegetable capsicum\", \"vegetable corn\",\n",
    "    \"vegetable cauliflower\", \"vegetable beetroot\", \"vegetable bell pepper\", \"vegetable chili pepper\"\n",
    "]\n",
    "\n",
    "# Start crawling for each category\n",
    "for category in categories:\n",
    "    folder_name = category.split()[-1]\n",
    "    crawl_images(keyword=category, save_dir=f\"./images/{folder_name}\", count=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
